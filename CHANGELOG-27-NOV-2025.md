# ğŸ“ CHANGELOG â€” 27 Novembre 2025

## ğŸš€ Versione 2.1 â€” Model Management Update

### âœ¨ Nuove FunzionalitÃ 

#### ğŸ¯ Gestione Modelli Completa
- **`/models`** â€” Lista tutti i modelli disponibili con stato (installato/non installato)
- **`/installed`** â€” Mostra modelli installati con dimensione occupata su disco
- **`/download <model>`** â€” Scarica modelli on-demand senza riavviare
- **`/delete <model>`** â€” Elimina modelli per liberare spazio disco

#### ğŸ“¦ 20+ Modelli Ottimizzati M4 Pro 24GB

**Qwen Coder (Best for Code):**
- `/q1.5b` â€” Qwen 1.5B (~1GB) - Quick testing
- `/q3b` â€” Qwen 3B (~2GB) - Fast coding
- `/q7b` â€” Qwen 7B (~4GB) - Recommended
- `/q14b` â€” Qwen 14B (~9GB) - Advanced
- **`/q32b`** â€” **Qwen 32B (~17GB) - Best Quality** â­ NEW!

**DeepSeek Coder (Excellent):**
- `/ds1.3b` â€” DeepSeek 1.3B (~1GB)
- `/ds6.7b` â€” DeepSeek 6.7B (~4GB)
- `/ds` â€” DeepSeek V2 Lite (~9GB)

**Llama 3 (Strong Reasoning):**
- `/llama3-8b` â€” Llama 3 8B (~5GB) â­ NEW!
- `/l3-8b` â€” Alias per Llama 3 8B

**Phi (Efficient):**
- `/phi3` â€” Phi-3 Mini (~2GB) â­ NEW!
- `/phi` â€” Alias per Phi-3

**CodeLlama (Code Specialist):**
- `/codellama` â€” CodeLlama 13B (~7GB) â­ NEW!
- `/cl13b` â€” Alias per CodeLlama 13B

**Mistral (Versatile):**
- `/mistral` â€” Mistral 7B (~4GB)
- `/m7b` â€” Alias per Mistral 7B

#### âŒ¨ï¸ Input Terminale Avanzato (prompt-toolkit)
- **Cronologia Comandi** â€” Naviga con â†‘/â†“ tra comandi precedenti
- **Navigazione Cursore** â€” Muovi il cursore con â†/â†’ per editare
- **Tab Completion** â€” Auto-completa comandi (es: `/mod` + Tab â†’ `/models`)
- **Multi-line Paste** â€” Incolla codice multi-riga senza problemi
- **Smart Ctrl+C** â€” Pulisce il buffer senza mostrare ^C
- **Storia Persistente** â€” Salva tutti i comandi in `~/.mlx-code/command_history.txt`

**Installazione:**
```bash
pip install prompt-toolkit
# oppure
pip install -r requirements.txt
```

**Benefici:**
- âœ… Niente piÃ¹ simboli strani quando usi le frecce
- âœ… Niente piÃ¹ problemi quando incolli codice
- âœ… Ctrl+C funziona correttamente (non mostra ^C)
- âœ… Esperienza professionale come zsh/bash moderni

### ğŸ”§ Miglioramenti Tecnici

#### Model Helpers
```python
# Nuove funzioni aggiunte:
- list_installed_models() â†’ Lista modelli installati
- delete_model(name) â†’ Elimina modello da cache
- get_model_ram_requirement(name) â†’ Stima RAM necessaria
- list_available_models() â†’ Metadata completi di tutti i modelli
- get_model_size_estimate(name) â†’ Stima dimensione download migliorata
```

#### Import Mancante
- Aggiunto `import subprocess` necessario per `download_model_with_git_lfs()`

#### Alias Dinamici
- Sistema di alias completamente dinamico: tutti gli alias in `MODEL_ALIASES` funzionano automaticamente
- Non serve piÃ¹ hardcodare `/q7b`, `/q3b` etc nel codice

### ğŸ“š Nuova Documentazione

#### **GUIDA-M4-PRO-24GB.md** (NUOVO!)
Guida completa in italiano per sfruttare M4 Pro con 24GB RAM:
- Panoramica completa modelli
- Comandi e esempi pratici
- Comparazione prestazioni
- Setup ottimale raccomandato
- Tips & tricks per M4 Pro

#### Sezioni Aggiunte:
- Quando usare quale modello
- Workflow ottimizzati
- FAQ M4 Pro specific
- Setup a 3 modelli complementari

### ğŸ¨ Miglioramenti UI

#### Help Aggiornato
```
MODEL & SETTINGS:
  /model <id>            Switch model
  /models                List available models      â­ NEW
  /installed             Show installed models      â­ NEW
  /download <model>      Download a model          â­ NEW
  /delete <model>        Delete a model from cache â­ NEW

  Quick model switches (M4 Pro 24GB optimized):
    /q1.5b (1GB)   /q3b (2GB)    /q7b (4GB)    /q14b (9GB)   /q32b (17GB)
    /ds1.3b (1GB)  /ds6.7b (4GB)  /ds (9GB)     /deepseek (9GB)
    /phi3 (2GB)    /llama3-8b (5GB)  /mistral (4GB)  /codellama (7GB)
```

#### Comando `/models` Output
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“¦ Available Models (M4 Pro 24GB Optimized)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Qwen Coder (Recommended for Code)
  /q1.5b          ~1.0GB   ~2-3GB RAM  âœ“ Installed
  /q3b            ~1.9GB   ~3-4GB RAM  âœ— Not installed
  /q7b            ~4.3GB   ~5-7GB RAM  âœ“ Installed
  /q14b           ~8.5GB  ~10-12GB RAM  âœ— Not installed
  /q32b          ~17.0GB  ~20-22GB RAM  âœ“ Installed

[... altri modelli ...]

ğŸ’¡ Usage:
  â€¢ Switch model: /<alias> (e.g., /q32b)
  â€¢ Download: /download <alias> (e.g., /download q32b)
  â€¢ Delete: /delete <alias>
```

#### Comando `/installed` Output
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ’¾ Installed Models
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  mlx-community/Qwen2.5-Coder-1.5B-Instruct-4bit              1.02GB  /q1.5b
  mlx-community/qwen2.5-coder-7b-instruct-4bit                4.28GB  /q7b
  mlx-community/Qwen2.5-Coder-32B-Instruct-4bit              17.34GB  /q32b

Total disk usage: 22.64GB
Cache location: ~/.cache/huggingface/hub/
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### ğŸ› Bug Fix

- Risolto: `subprocess` non importato causava errore in `download_model_with_git_lfs()`
- Risolto: Stima dimensione modelli imprecisa per modelli > 14B

### ğŸ’¡ Breaking Changes

Nessuno! Tutte le nuove funzionalitÃ  sono backwards-compatible.

### ğŸ“Š Statistiche

- **+20 modelli** disponibili (prima: 9, ora: 29)
- **+4 comandi** per gestione modelli
- **+100 righe** di codice gestione modelli
- **+500 righe** di documentazione (GUIDE-M4-PRO-24GB.md + GUIDE-M1-16GB.md)
- **+1 dipendenza opzionale** (prompt-toolkit per input avanzato)
- **+50 righe** per integrazione prompt-toolkit con fallback graceful

---

## ğŸ¯ Come Aggiornare

### Se hai giÃ  mlx-code installato:

```bash
cd ~/Projects/MLX-Terminal-Code
git pull origin main

# Installa dipendenze aggiornate (raccomandato)
source ~/.mlx-env/bin/activate
pip install -r requirements.txt
# oppure solo prompt-toolkit:
pip install prompt-toolkit

# Copia nuova versione
cp mlx-code-v2.py ~/mlx-code
chmod +x ~/mlx-code

# Testa
~/mlx-code
> /models
```

### Prima Installazione:

```bash
cd ~/Projects/MLX-Terminal-Code

# Installa dipendenze (raccomandato)
source ~/.mlx-env/bin/activate
pip install -r requirements.txt

# Copia e attiva mlx-code
cp mlx-code-v2.py ~/mlx-code
chmod +x ~/mlx-code
~/mlx-code
```

---

## ğŸš€ Quick Start per M4 Pro 24GB

```bash
# Avvia mlx-code
~/mlx-code

# Vedi tutti i modelli
> /models

# Scarica il piÃ¹ potente (Qwen 32B)
> /download q32b
# Attendi ~15 minuti

# Passa a Qwen 32B
> /q32b

# Oppure scarica DeepSeek (piÃ¹ veloce)
> /download ds
> /ds
```

---

## ğŸ“ˆ Prossime Features (Roadmap)

- [ ] Confronto automatico tra modelli (`/benchmark`)
- [ ] Auto-switch basato su complessitÃ  query
- [ ] Download in background
- [ ] Supporto modelli custom
- [ ] Interface web (opzionale)
- [ ] Model zoo integrato

---

## ğŸ™ Contributors

- **Gianluca** â€” Model management system & M4 Pro optimization
- **Claude** â€” Documentation & testing

---

## ğŸ“ Note di Rilascio

**Data:** 27 Novembre 2025
**Versione:** 2.1.0
**Python:** 3.12+
**MLX:** 0.20.0+
**Compatibile:** M1/M2/M3/M4 (8GB/16GB/24GB/48GB RAM)

---

**ğŸ‰ Buon coding con mlx-code v2.1!**
